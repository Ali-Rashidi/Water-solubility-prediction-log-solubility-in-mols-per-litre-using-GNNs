{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bba4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from torch_geometric.datasets import MoleculeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719db7b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f11ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MoleculeNet(root = ',',name='ESOL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa159839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Type :  <class 'torch_geometric.datasets.molecule_net.MoleculeNet'>\n",
      "Dataset Length :  <bound method InMemoryDataset.len of ESOL(1128)>\n",
      "Dataset Features :  9\n",
      "Dataset Classes :  734\n",
      "Dataset Sample :  Data(x=[32, 9], edge_index=[2, 68], edge_attr=[68, 3], smiles='OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)C(O)C3O ', y=[1, 1])\n",
      "Sample Nodes : 32\n",
      "Sample Edges : 68\n"
     ]
    }
   ],
   "source": [
    "## Graph Level Prediction : Regression\n",
    "print(\"Dataset Type : \" , type(data))\n",
    "print(\"Dataset Length : \" , data.len)\n",
    "print(\"Dataset Features : \" , data.num_features)\n",
    "print(\"Dataset Classes : \" , data.num_classes)\n",
    "print(\"Dataset Sample : \" , data[0])\n",
    "print(\"Sample Nodes :\" , data[0].num_nodes)\n",
    "print(\"Sample Edges :\" , data[0].num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c696da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 0, 2, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 2, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 0, 0, 4, 0, 1],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 0, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 2, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 0, 0, 4, 0, 1],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 0, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 2, 5, 0, 0, 2, 0, 0],\n",
       "        [7, 0, 1, 5, 0, 0, 2, 0, 0],\n",
       "        [6, 0, 3, 5, 0, 0, 3, 1, 1],\n",
       "        [6, 0, 3, 5, 1, 0, 3, 1, 1],\n",
       "        [6, 0, 3, 5, 1, 0, 3, 1, 1],\n",
       "        [6, 0, 3, 5, 1, 0, 3, 1, 1],\n",
       "        [6, 0, 3, 5, 1, 0, 3, 1, 1],\n",
       "        [6, 0, 3, 5, 1, 0, 3, 1, 1],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 1, 0, 4, 0, 0],\n",
       "        [6, 0, 4, 5, 1, 0, 4, 0, 1],\n",
       "        [8, 0, 2, 5, 1, 0, 4, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edge features :\n",
    "data[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d004c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 8],\n",
       "        [1, 0],\n",
       "        [1, 2],\n",
       "        [2, 1],\n",
       "        [2, 3],\n",
       "        [3, 2],\n",
       "        [3, 4],\n",
       "        [3, 7],\n",
       "        [4, 3],\n",
       "        [4, 5],\n",
       "        [5, 4],\n",
       "        [5, 6],\n",
       "        [6, 5],\n",
       "        [6, 7],\n",
       "        [7, 3],\n",
       "        [7, 6],\n",
       "        [7, 8],\n",
       "        [8, 0],\n",
       "        [8, 7]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## edge information (tuple 0f connected nodes)\n",
    "data[5].edge_index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca90f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3200]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Graph output \n",
    "data[6].y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca04f9c",
   "metadata": {},
   "source": [
    "# Implementing GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "16d24125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (initial_conv): GCNConv(9, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (do1): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  14421\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear , Dropout\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "embedding_size = 64\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "        self.do1 = Dropout(p=0.5)\n",
    "        # FC layer\n",
    "        self.fc = Linear(embedding_size*2, 10)\n",
    "        self.out = Linear(10, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "          \n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply FC Layers :\n",
    "        hid = self.do1(hidden)\n",
    "        hid = self.fc(hid)\n",
    "        hid = F.tanh(hid)\n",
    "        out = self.out(hid)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "12c2d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Train Loss 7.160151958465576 | Val Loss 9.313027381896973\n",
      "Epoch 10 | Train Loss 4.654757976531982 | Val Loss 5.780024528503418\n",
      "Epoch 20 | Train Loss 3.407512664794922 | Val Loss 4.859036445617676\n",
      "Epoch 30 | Train Loss 5.059898853302002 | Val Loss 4.994475364685059\n",
      "Epoch 40 | Train Loss 4.2903923988342285 | Val Loss 4.999227523803711\n",
      "Epoch 50 | Train Loss 5.400219440460205 | Val Loss 5.340348243713379\n",
      "Epoch 60 | Train Loss 2.4192216396331787 | Val Loss 4.689610481262207\n",
      "Epoch 70 | Train Loss 3.891489267349243 | Val Loss 4.85308837890625\n",
      "Epoch 80 | Train Loss 4.899650573730469 | Val Loss 5.114058017730713\n",
      "Epoch 90 | Train Loss 4.059142112731934 | Val Loss 4.894408702850342\n",
      "Epoch 100 | Train Loss 5.429331302642822 | Val Loss 4.972578048706055\n",
      "Epoch 110 | Train Loss 3.3876190185546875 | Val Loss 4.687424659729004\n",
      "Epoch 120 | Train Loss 4.014639854431152 | Val Loss 4.8972954750061035\n",
      "Epoch 130 | Train Loss 2.2254996299743652 | Val Loss 4.753316879272461\n",
      "Epoch 140 | Train Loss 4.199770927429199 | Val Loss 4.91187047958374\n",
      "Epoch 150 | Train Loss 6.386620044708252 | Val Loss 4.8201470375061035\n",
      "Epoch 160 | Train Loss 4.085779666900635 | Val Loss 4.795560359954834\n",
      "Epoch 170 | Train Loss 5.837705612182617 | Val Loss 5.109958648681641\n",
      "Epoch 180 | Train Loss 4.658662796020508 | Val Loss 4.609930038452148\n",
      "Epoch 190 | Train Loss 4.942863464355469 | Val Loss 3.988158702850342\n",
      "Epoch 200 | Train Loss 1.309524416923523 | Val Loss 2.9333529472351074\n",
      "Epoch 210 | Train Loss 2.816664934158325 | Val Loss 2.710212469100952\n",
      "Epoch 220 | Train Loss 1.8736640214920044 | Val Loss 2.430161952972412\n",
      "Epoch 230 | Train Loss 1.546486496925354 | Val Loss 2.1707069873809814\n",
      "Epoch 240 | Train Loss 1.8261620998382568 | Val Loss 1.795583724975586\n",
      "Epoch 250 | Train Loss 1.6764600276947021 | Val Loss 1.9891915321350098\n",
      "Epoch 260 | Train Loss 2.2273716926574707 | Val Loss 1.6650198698043823\n",
      "Epoch 270 | Train Loss 0.857499897480011 | Val Loss 1.672616958618164\n",
      "Epoch 280 | Train Loss 1.1139172315597534 | Val Loss 1.568123459815979\n",
      "Epoch 290 | Train Loss 0.7003530859947205 | Val Loss 1.5794670581817627\n",
      "Epoch 300 | Train Loss 1.475206971168518 | Val Loss 1.7605645656585693\n",
      "Epoch 310 | Train Loss 0.9405517578125 | Val Loss 1.565887451171875\n",
      "Epoch 320 | Train Loss 1.487282395362854 | Val Loss 1.5526636838912964\n",
      "Epoch 330 | Train Loss 1.0866905450820923 | Val Loss 1.3960784673690796\n",
      "Epoch 340 | Train Loss 1.0410276651382446 | Val Loss 1.3900707960128784\n",
      "Epoch 350 | Train Loss 0.8525696992874146 | Val Loss 1.4859429597854614\n",
      "Epoch 360 | Train Loss 0.6686270833015442 | Val Loss 1.172905445098877\n",
      "Epoch 370 | Train Loss 0.761498212814331 | Val Loss 1.251354455947876\n",
      "Epoch 380 | Train Loss 1.1346969604492188 | Val Loss 1.3343908786773682\n",
      "Epoch 390 | Train Loss 0.4364544451236725 | Val Loss 1.4335474967956543\n",
      "Epoch 400 | Train Loss 0.5011070966720581 | Val Loss 1.2820048332214355\n",
      "Epoch 410 | Train Loss 0.6495928168296814 | Val Loss 1.2095791101455688\n",
      "Epoch 420 | Train Loss 0.7436418533325195 | Val Loss 1.1316481828689575\n",
      "Epoch 430 | Train Loss 0.6294922828674316 | Val Loss 1.1498180627822876\n",
      "Epoch 440 | Train Loss 0.42542797327041626 | Val Loss 1.2241230010986328\n",
      "Epoch 450 | Train Loss 0.7976171970367432 | Val Loss 1.0217888355255127\n",
      "Epoch 460 | Train Loss 0.6837013363838196 | Val Loss 1.1333324909210205\n",
      "Epoch 470 | Train Loss 0.43963512778282166 | Val Loss 1.0885025262832642\n",
      "Epoch 480 | Train Loss 0.6594921350479126 | Val Loss 1.1279246807098389\n",
      "Epoch 490 | Train Loss 0.5879698991775513 | Val Loss 0.9572848081588745\n",
      "Epoch 500 | Train Loss 0.8612060546875 | Val Loss 0.9951361417770386\n",
      "Epoch 510 | Train Loss 0.5864477753639221 | Val Loss 1.0053571462631226\n",
      "Epoch 520 | Train Loss 0.8396454453468323 | Val Loss 0.9539598822593689\n",
      "Epoch 530 | Train Loss 0.5192842483520508 | Val Loss 1.0186808109283447\n",
      "Epoch 540 | Train Loss 0.799022376537323 | Val Loss 0.994293212890625\n",
      "Epoch 550 | Train Loss 0.7374384999275208 | Val Loss 0.9573692083358765\n",
      "Epoch 560 | Train Loss 0.5575979351997375 | Val Loss 1.0363330841064453\n",
      "Epoch 570 | Train Loss 0.4677821397781372 | Val Loss 1.0909299850463867\n",
      "Epoch 580 | Train Loss 0.649821400642395 | Val Loss 0.8999913930892944\n",
      "Epoch 590 | Train Loss 0.5029862523078918 | Val Loss 0.9752260446548462\n",
      "Epoch 600 | Train Loss 0.43224215507507324 | Val Loss 1.0938138961791992\n",
      "Epoch 610 | Train Loss 0.5572847723960876 | Val Loss 0.8717563152313232\n",
      "Epoch 620 | Train Loss 0.4953884482383728 | Val Loss 0.9770854115486145\n",
      "Epoch 630 | Train Loss 0.31728506088256836 | Val Loss 0.9017187356948853\n",
      "Epoch 640 | Train Loss 0.6339719891548157 | Val Loss 1.1484355926513672\n",
      "Epoch 650 | Train Loss 0.4642035663127899 | Val Loss 0.9237344861030579\n",
      "Epoch 660 | Train Loss 0.4977431893348694 | Val Loss 1.0692801475524902\n",
      "Epoch 670 | Train Loss 0.9800825119018555 | Val Loss 0.8912200927734375\n",
      "Epoch 680 | Train Loss 0.7563261985778809 | Val Loss 0.8998465538024902\n",
      "Epoch 690 | Train Loss 0.41758298873901367 | Val Loss 0.8727827668190002\n",
      "Epoch 700 | Train Loss 0.5621997117996216 | Val Loss 0.8467916250228882\n",
      "Epoch 710 | Train Loss 0.4764493405818939 | Val Loss 0.8304243683815002\n",
      "Epoch 720 | Train Loss 0.6130046844482422 | Val Loss 0.953094482421875\n",
      "Epoch 730 | Train Loss 0.3978506326675415 | Val Loss 0.9979870915412903\n",
      "Epoch 740 | Train Loss 0.43644437193870544 | Val Loss 0.8958265781402588\n",
      "Epoch 750 | Train Loss 0.6735654473304749 | Val Loss 0.8164037466049194\n",
      "Epoch 760 | Train Loss 0.6172254681587219 | Val Loss 0.9092103242874146\n",
      "Epoch 770 | Train Loss 0.6215165257453918 | Val Loss 0.8914644718170166\n",
      "Epoch 780 | Train Loss 0.48060736060142517 | Val Loss 0.7512113451957703\n",
      "Epoch 790 | Train Loss 0.47469085454940796 | Val Loss 0.7983090877532959\n",
      "Epoch 800 | Train Loss 0.8001046180725098 | Val Loss 0.9065403938293457\n",
      "Epoch 810 | Train Loss 0.7581691145896912 | Val Loss 0.9607201814651489\n",
      "Epoch 820 | Train Loss 0.7432447075843811 | Val Loss 0.8815937638282776\n",
      "Epoch 830 | Train Loss 0.46607092022895813 | Val Loss 1.0267757177352905\n",
      "Epoch 840 | Train Loss 0.4923696517944336 | Val Loss 0.8990402817726135\n",
      "Epoch 850 | Train Loss 0.4786653518676758 | Val Loss 0.8217730522155762\n",
      "Epoch 860 | Train Loss 0.504905104637146 | Val Loss 0.8294860124588013\n",
      "Epoch 870 | Train Loss 0.48184332251548767 | Val Loss 0.8349466323852539\n",
      "Epoch 880 | Train Loss 0.44801411032676697 | Val Loss 0.8633166551589966\n",
      "Epoch 890 | Train Loss 0.5709249973297119 | Val Loss 0.8394452333450317\n",
      "Epoch 900 | Train Loss 0.4856376647949219 | Val Loss 0.8610328435897827\n",
      "Epoch 910 | Train Loss 0.51032954454422 | Val Loss 1.098628282546997\n",
      "Epoch 920 | Train Loss 0.6368539929389954 | Val Loss 0.8132463097572327\n",
      "Epoch 930 | Train Loss 0.4080527722835541 | Val Loss 0.7185125350952148\n",
      "Epoch 940 | Train Loss 0.548543393611908 | Val Loss 0.9214547872543335\n",
      "Epoch 950 | Train Loss 0.7281340956687927 | Val Loss 0.8444066047668457\n",
      "Epoch 960 | Train Loss 0.7583799958229065 | Val Loss 0.9260542392730713\n",
      "Epoch 970 | Train Loss 0.28823745250701904 | Val Loss 0.8572559952735901\n",
      "Epoch 980 | Train Loss 0.4102981686592102 | Val Loss 0.7447503209114075\n",
      "Epoch 990 | Train Loss 0.49154895544052124 | Val Loss 0.7998952269554138\n",
      "Epoch 1000 | Train Loss 0.568503737449646 | Val Loss 0.9031511545181274\n",
      "Epoch 1010 | Train Loss 0.38631677627563477 | Val Loss 0.8264559507369995\n",
      "Epoch 1020 | Train Loss 0.4747253656387329 | Val Loss 0.7906072735786438\n",
      "Epoch 1030 | Train Loss 0.5175293684005737 | Val Loss 0.7270233631134033\n",
      "Epoch 1040 | Train Loss 0.6179530024528503 | Val Loss 0.8286513686180115\n",
      "Epoch 1050 | Train Loss 0.4183931350708008 | Val Loss 0.9269422292709351\n",
      "Epoch 1060 | Train Loss 0.45806801319122314 | Val Loss 0.818886399269104\n",
      "Epoch 1070 | Train Loss 0.7608261108398438 | Val Loss 0.7864740490913391\n",
      "Epoch 1080 | Train Loss 0.2815489172935486 | Val Loss 0.7814472913742065\n",
      "Epoch 1090 | Train Loss 0.38535401225090027 | Val Loss 0.74587482213974\n",
      "Epoch 1100 | Train Loss 0.6426251530647278 | Val Loss 0.6887913942337036\n",
      "Epoch 1110 | Train Loss 0.8588975071907043 | Val Loss 0.6909863352775574\n",
      "Epoch 1120 | Train Loss 0.63032466173172 | Val Loss 0.7608954906463623\n",
      "Epoch 1130 | Train Loss 0.5872206091880798 | Val Loss 0.8444598317146301\n",
      "Epoch 1140 | Train Loss 0.5390455722808838 | Val Loss 0.8535516262054443\n",
      "Epoch 1150 | Train Loss 0.536007285118103 | Val Loss 0.7574713826179504\n",
      "Epoch 1160 | Train Loss 0.32153406739234924 | Val Loss 0.8328312635421753\n",
      "Epoch 1170 | Train Loss 0.5244442820549011 | Val Loss 0.8318889737129211\n",
      "Epoch 1180 | Train Loss 0.621557354927063 | Val Loss 0.6585357189178467\n",
      "Epoch 1190 | Train Loss 0.4172205924987793 | Val Loss 0.7844492197036743\n",
      "Epoch 1200 | Train Loss 0.7880972623825073 | Val Loss 0.7136068940162659\n",
      "Epoch 1210 | Train Loss 0.23937208950519562 | Val Loss 0.6817355751991272\n",
      "Epoch 1220 | Train Loss 0.43320927023887634 | Val Loss 0.7625319957733154\n",
      "Epoch 1230 | Train Loss 0.3130611777305603 | Val Loss 0.7330931425094604\n",
      "Epoch 1240 | Train Loss 0.36685383319854736 | Val Loss 0.724522054195404\n",
      "Epoch 1250 | Train Loss 0.34154945611953735 | Val Loss 0.7924073338508606\n",
      "Epoch 1260 | Train Loss 0.28523799777030945 | Val Loss 0.8021437525749207\n",
      "Epoch 1270 | Train Loss 0.3536997437477112 | Val Loss 0.7611576318740845\n",
      "Epoch 1280 | Train Loss 0.3085189163684845 | Val Loss 0.731072723865509\n",
      "Epoch 1290 | Train Loss 0.571251630783081 | Val Loss 0.8077866435050964\n",
      "Epoch 1300 | Train Loss 0.3411596119403839 | Val Loss 0.8244791030883789\n",
      "Epoch 1310 | Train Loss 0.37195199728012085 | Val Loss 0.8376572132110596\n",
      "Epoch 1320 | Train Loss 0.47478049993515015 | Val Loss 0.7831329703330994\n",
      "Epoch 1330 | Train Loss 0.40402963757514954 | Val Loss 0.7480024099349976\n",
      "Epoch 1340 | Train Loss 0.3880764842033386 | Val Loss 0.6874109506607056\n",
      "Epoch 1350 | Train Loss 0.3554449677467346 | Val Loss 0.8156209588050842\n",
      "Epoch 1360 | Train Loss 0.4419136643409729 | Val Loss 0.8274229764938354\n",
      "Epoch 1370 | Train Loss 0.29036104679107666 | Val Loss 0.7073028683662415\n",
      "Epoch 1380 | Train Loss 0.4247454106807709 | Val Loss 0.7067683935165405\n",
      "Epoch 1390 | Train Loss 0.3425418436527252 | Val Loss 0.6975915431976318\n",
      "Epoch 1400 | Train Loss 0.3504738211631775 | Val Loss 0.6867067813873291\n",
      "Epoch 1410 | Train Loss 0.28814226388931274 | Val Loss 0.7565587759017944\n",
      "Epoch 1420 | Train Loss 0.42204347252845764 | Val Loss 0.6901320815086365\n",
      "Epoch 1430 | Train Loss 0.34376654028892517 | Val Loss 0.7869769334793091\n",
      "Epoch 1440 | Train Loss 0.455312579870224 | Val Loss 0.8656904101371765\n",
      "Epoch 1450 | Train Loss 0.28767600655555725 | Val Loss 0.7090121507644653\n",
      "Epoch 1460 | Train Loss 0.2432805448770523 | Val Loss 0.7199069261550903\n",
      "Epoch 1470 | Train Loss 0.30794668197631836 | Val Loss 0.9075431227684021\n",
      "Epoch 1480 | Train Loss 0.43922388553619385 | Val Loss 0.6968562602996826\n",
      "Epoch 1490 | Train Loss 0.4847298264503479 | Val Loss 0.7422996759414673\n",
      "Epoch 1500 | Train Loss 0.27500900626182556 | Val Loss 0.8404355049133301\n",
      "Epoch 1510 | Train Loss 0.4944567084312439 | Val Loss 0.7466896176338196\n",
      "Epoch 1520 | Train Loss 0.34761565923690796 | Val Loss 0.7621527314186096\n",
      "Epoch 1530 | Train Loss 0.5155506134033203 | Val Loss 0.8824350833892822\n",
      "Epoch 1540 | Train Loss 0.39248600602149963 | Val Loss 0.6770351529121399\n",
      "Epoch 1550 | Train Loss 0.3209775984287262 | Val Loss 0.7837099432945251\n",
      "Epoch 1560 | Train Loss 0.5696437954902649 | Val Loss 0.7523683309555054\n",
      "Epoch 1570 | Train Loss 0.5731541514396667 | Val Loss 0.6938921809196472\n",
      "Epoch 1580 | Train Loss 0.5874369740486145 | Val Loss 0.8516469597816467\n",
      "Epoch 1590 | Train Loss 0.5752629041671753 | Val Loss 0.6685617566108704\n",
      "Epoch 1600 | Train Loss 0.203807532787323 | Val Loss 0.7642573118209839\n",
      "Epoch 1610 | Train Loss 0.4204292893409729 | Val Loss 0.7161424160003662\n",
      "Epoch 1620 | Train Loss 0.5339499711990356 | Val Loss 0.7256382703781128\n",
      "Epoch 1630 | Train Loss 0.37529125809669495 | Val Loss 0.8672255277633667\n",
      "Epoch 1640 | Train Loss 0.38838523626327515 | Val Loss 0.8113332390785217\n",
      "Epoch 1650 | Train Loss 0.2858368158340454 | Val Loss 0.6734241247177124\n",
      "Epoch 1660 | Train Loss 0.3456108272075653 | Val Loss 0.7036965489387512\n",
      "Epoch 1670 | Train Loss 0.2363007813692093 | Val Loss 0.6867815852165222\n",
      "Epoch 1680 | Train Loss 0.525976836681366 | Val Loss 0.7348635196685791\n",
      "Epoch 1690 | Train Loss 0.4356755316257477 | Val Loss 0.7070645093917847\n",
      "Epoch 1700 | Train Loss 0.24407263100147247 | Val Loss 0.7019265294075012\n",
      "Epoch 1710 | Train Loss 0.7256550192832947 | Val Loss 0.7160505056381226\n",
      "Epoch 1720 | Train Loss 0.20299570262432098 | Val Loss 0.6732082962989807\n",
      "Epoch 1730 | Train Loss 0.29088228940963745 | Val Loss 0.7211776971817017\n",
      "Epoch 1740 | Train Loss 0.23454448580741882 | Val Loss 0.7045781016349792\n",
      "Epoch 1750 | Train Loss 0.33222830295562744 | Val Loss 0.7408722639083862\n",
      "Epoch 1760 | Train Loss 0.22166316211223602 | Val Loss 0.6447059512138367\n",
      "Epoch 1770 | Train Loss 0.32777589559555054 | Val Loss 0.660189688205719\n",
      "Epoch 1780 | Train Loss 0.32085880637168884 | Val Loss 0.7319373488426208\n",
      "Epoch 1790 | Train Loss 0.18474185466766357 | Val Loss 0.8288182020187378\n",
      "Epoch 1800 | Train Loss 0.5006157755851746 | Val Loss 0.6615111231803894\n",
      "Epoch 1810 | Train Loss 0.3358568847179413 | Val Loss 0.7175800800323486\n",
      "Epoch 1820 | Train Loss 0.3398764431476593 | Val Loss 0.7540242671966553\n",
      "Epoch 1830 | Train Loss 0.4446598291397095 | Val Loss 0.6733647584915161\n",
      "Epoch 1840 | Train Loss 0.31899410486221313 | Val Loss 0.7574436664581299\n",
      "Epoch 1850 | Train Loss 0.8510282635688782 | Val Loss 0.885199248790741\n",
      "Epoch 1860 | Train Loss 0.23293596506118774 | Val Loss 0.7158806324005127\n",
      "Epoch 1870 | Train Loss 0.2877914011478424 | Val Loss 0.734021008014679\n",
      "Epoch 1880 | Train Loss 0.28205031156539917 | Val Loss 0.6827210783958435\n",
      "Epoch 1890 | Train Loss 0.2624795138835907 | Val Loss 0.7874892354011536\n",
      "Epoch 1900 | Train Loss 0.48699942231178284 | Val Loss 0.8779058456420898\n",
      "Epoch 1910 | Train Loss 0.24255116283893585 | Val Loss 0.7270118594169617\n",
      "Epoch 1920 | Train Loss 0.5455489158630371 | Val Loss 0.7615293264389038\n",
      "Epoch 1930 | Train Loss 0.5179318189620972 | Val Loss 0.709490180015564\n",
      "Epoch 1940 | Train Loss 0.22536110877990723 | Val Loss 0.7024768590927124\n",
      "Epoch 1950 | Train Loss 0.3471723198890686 | Val Loss 0.7283111810684204\n",
      "Epoch 1960 | Train Loss 0.3129420876502991 | Val Loss 0.7486890554428101\n",
      "Epoch 1970 | Train Loss 0.27316588163375854 | Val Loss 0.702119767665863\n",
      "Epoch 1980 | Train Loss 0.4780755043029785 | Val Loss 0.6856503486633301\n",
      "Epoch 1990 | Train Loss 0.2870345115661621 | Val Loss 0.8289116621017456\n",
      "Epoch 2000 | Train Loss 0.17307502031326294 | Val Loss 0.7365328073501587\n",
      "Epoch 2010 | Train Loss 0.4479976296424866 | Val Loss 0.7208094596862793\n",
      "Epoch 2020 | Train Loss 0.3061216473579407 | Val Loss 0.7059911489486694\n",
      "Epoch 2030 | Train Loss 0.22413988411426544 | Val Loss 0.6863712072372437\n",
      "Epoch 2040 | Train Loss 0.39089760184288025 | Val Loss 0.7246162295341492\n",
      "Epoch 2050 | Train Loss 0.43269655108451843 | Val Loss 0.6855374574661255\n",
      "Epoch 2060 | Train Loss 0.32866647839546204 | Val Loss 0.6625236868858337\n",
      "Epoch 2070 | Train Loss 0.3271586298942566 | Val Loss 0.8073229789733887\n",
      "Epoch 2080 | Train Loss 0.31493550539016724 | Val Loss 0.8055652379989624\n",
      "Epoch 2090 | Train Loss 0.48038890957832336 | Val Loss 0.754112958908081\n",
      "Epoch 2100 | Train Loss 0.219615176320076 | Val Loss 0.6404336094856262\n",
      "Epoch 2110 | Train Loss 0.27481338381767273 | Val Loss 0.7702653408050537\n",
      "Epoch 2120 | Train Loss 0.23062345385551453 | Val Loss 0.7413605451583862\n",
      "Epoch 2130 | Train Loss 0.22172079980373383 | Val Loss 0.8097540736198425\n",
      "Epoch 2140 | Train Loss 0.37614715099334717 | Val Loss 0.7120928764343262\n",
      "Epoch 2150 | Train Loss 0.1981627494096756 | Val Loss 0.710295557975769\n",
      "Epoch 2160 | Train Loss 0.25606071949005127 | Val Loss 0.6967758536338806\n",
      "Epoch 2170 | Train Loss 0.2339705377817154 | Val Loss 0.7489750385284424\n",
      "Epoch 2180 | Train Loss 0.1531420797109604 | Val Loss 0.7315377593040466\n",
      "Epoch 2190 | Train Loss 0.367510050535202 | Val Loss 0.6508213877677917\n",
      "Epoch 2200 | Train Loss 0.13659147918224335 | Val Loss 0.6978203654289246\n",
      "Epoch 2210 | Train Loss 0.23893359303474426 | Val Loss 0.7159253358840942\n",
      "Epoch 2220 | Train Loss 0.5039915442466736 | Val Loss 0.7404892444610596\n",
      "Epoch 2230 | Train Loss 0.34554389119148254 | Val Loss 0.7255977392196655\n",
      "Epoch 2240 | Train Loss 0.20490588247776031 | Val Loss 0.6700351238250732\n",
      "Epoch 2250 | Train Loss 0.2710191309452057 | Val Loss 0.6266160607337952\n",
      "Epoch 2260 | Train Loss 0.11926157027482986 | Val Loss 0.7019758820533752\n",
      "Epoch 2270 | Train Loss 0.32022857666015625 | Val Loss 0.6786890625953674\n",
      "Epoch 2280 | Train Loss 0.3318420648574829 | Val Loss 0.724694550037384\n",
      "Epoch 2290 | Train Loss 0.3390977084636688 | Val Loss 0.6901187300682068\n",
      "Epoch 2300 | Train Loss 0.2670772969722748 | Val Loss 0.7450549602508545\n",
      "Epoch 2310 | Train Loss 0.11371133476495743 | Val Loss 0.7817703485488892\n",
      "Epoch 2320 | Train Loss 0.6167502999305725 | Val Loss 0.6424811482429504\n",
      "Epoch 2330 | Train Loss 0.3065465986728668 | Val Loss 0.6784358620643616\n",
      "Epoch 2340 | Train Loss 0.5057392716407776 | Val Loss 0.639204204082489\n",
      "Epoch 2350 | Train Loss 0.29583540558815 | Val Loss 0.7263622283935547\n",
      "Epoch 2360 | Train Loss 0.18398356437683105 | Val Loss 0.7431085705757141\n",
      "Epoch 2370 | Train Loss 0.2072068154811859 | Val Loss 0.660168468952179\n",
      "Epoch 2380 | Train Loss 0.27277350425720215 | Val Loss 0.7655696272850037\n",
      "Epoch 2390 | Train Loss 0.318094938993454 | Val Loss 0.7735182642936707\n",
      "Epoch 2400 | Train Loss 0.2477574646472931 | Val Loss 0.6632381677627563\n",
      "Epoch 2410 | Train Loss 0.28465718030929565 | Val Loss 0.7816833257675171\n",
      "Epoch 2420 | Train Loss 0.1785038709640503 | Val Loss 0.6457011103630066\n",
      "Epoch 2430 | Train Loss 0.23598773777484894 | Val Loss 0.8576600551605225\n",
      "Epoch 2440 | Train Loss 0.19383646547794342 | Val Loss 0.6741493344306946\n",
      "Epoch 2450 | Train Loss 0.21011993288993835 | Val Loss 0.7582342028617859\n",
      "Epoch 2460 | Train Loss 0.3725410997867584 | Val Loss 0.6513362526893616\n",
      "Epoch 2470 | Train Loss 0.19025461375713348 | Val Loss 0.7468453049659729\n",
      "Epoch 2480 | Train Loss 0.35773029923439026 | Val Loss 0.7090898752212524\n",
      "Epoch 2490 | Train Loss 0.39491769671440125 | Val Loss 0.6593877077102661\n",
      "Epoch 2500 | Train Loss 0.2748614251613617 | Val Loss 0.6754893064498901\n",
      "Epoch 2510 | Train Loss 0.21618607640266418 | Val Loss 0.7367146611213684\n",
      "Epoch 2520 | Train Loss 0.277444988489151 | Val Loss 0.6787369251251221\n",
      "Epoch 2530 | Train Loss 0.2284124195575714 | Val Loss 0.6710492968559265\n",
      "Epoch 2540 | Train Loss 0.3819246292114258 | Val Loss 0.6178725361824036\n",
      "Epoch 2550 | Train Loss 0.2562839090824127 | Val Loss 0.660612165927887\n",
      "Epoch 2560 | Train Loss 0.4043012261390686 | Val Loss 0.7150558233261108\n",
      "Epoch 2570 | Train Loss 0.21711689233779907 | Val Loss 0.6511777639389038\n",
      "Epoch 2580 | Train Loss 0.2134518176317215 | Val Loss 0.6529382467269897\n",
      "Epoch 2590 | Train Loss 0.27776989340782166 | Val Loss 0.712283730506897\n",
      "Epoch 2600 | Train Loss 0.271382212638855 | Val Loss 0.7055783271789551\n",
      "Epoch 2610 | Train Loss 0.1612452268600464 | Val Loss 0.6700097322463989\n",
      "Epoch 2620 | Train Loss 0.1189037561416626 | Val Loss 0.7156959772109985\n",
      "Epoch 2630 | Train Loss 0.3220333158969879 | Val Loss 0.7289161682128906\n",
      "Epoch 2640 | Train Loss 0.2613767683506012 | Val Loss 0.6802312135696411\n",
      "Epoch 2650 | Train Loss 0.1640700250864029 | Val Loss 0.7037941217422485\n",
      "Epoch 2660 | Train Loss 0.18590989708900452 | Val Loss 0.6063531041145325\n",
      "Epoch 2670 | Train Loss 0.26687946915626526 | Val Loss 0.651035726070404\n",
      "Epoch 2680 | Train Loss 0.29476627707481384 | Val Loss 0.7003318667411804\n",
      "Epoch 2690 | Train Loss 0.3105572760105133 | Val Loss 0.7170050740242004\n",
      "Epoch 2700 | Train Loss 0.3463359475135803 | Val Loss 0.7600213289260864\n",
      "Epoch 2710 | Train Loss 0.34506934881210327 | Val Loss 0.6450507044792175\n",
      "Epoch 2720 | Train Loss 0.30810272693634033 | Val Loss 0.6915148496627808\n",
      "Epoch 2730 | Train Loss 0.22423894703388214 | Val Loss 0.7325336933135986\n",
      "Epoch 2740 | Train Loss 0.18260642886161804 | Val Loss 0.7351913452148438\n",
      "Epoch 2750 | Train Loss 0.19914589822292328 | Val Loss 0.6609142422676086\n",
      "Epoch 2760 | Train Loss 0.4038378596305847 | Val Loss 0.7142642736434937\n",
      "Epoch 2770 | Train Loss 0.09830012917518616 | Val Loss 0.6982479095458984\n",
      "Epoch 2780 | Train Loss 0.25846657156944275 | Val Loss 0.7735270261764526\n",
      "Epoch 2790 | Train Loss 0.2160278856754303 | Val Loss 0.6661255955696106\n",
      "Epoch 2800 | Train Loss 0.3058868646621704 | Val Loss 0.8764625191688538\n",
      "Epoch 2810 | Train Loss 0.20011788606643677 | Val Loss 0.6774910688400269\n",
      "Epoch 2820 | Train Loss 0.1463508903980255 | Val Loss 0.638508141040802\n",
      "Epoch 2830 | Train Loss 0.20725709199905396 | Val Loss 0.7564722299575806\n",
      "Epoch 2840 | Train Loss 0.16890361905097961 | Val Loss 0.6138567924499512\n",
      "Epoch 2850 | Train Loss 0.26395073533058167 | Val Loss 0.7472288012504578\n",
      "Epoch 2860 | Train Loss 0.17652297019958496 | Val Loss 0.7309579253196716\n",
      "Epoch 2870 | Train Loss 0.18616332113742828 | Val Loss 0.6508491039276123\n",
      "Epoch 2880 | Train Loss 0.3177202641963959 | Val Loss 0.8017780184745789\n",
      "Epoch 2890 | Train Loss 0.1967829018831253 | Val Loss 0.6552973985671997\n",
      "Epoch 2900 | Train Loss 0.11713884770870209 | Val Loss 0.6557686924934387\n",
      "Epoch 2910 | Train Loss 0.20484614372253418 | Val Loss 0.6643369197845459\n",
      "Epoch 2920 | Train Loss 0.3171299993991852 | Val Loss 0.701291024684906\n",
      "Epoch 2930 | Train Loss 0.14792771637439728 | Val Loss 0.6847623586654663\n",
      "Epoch 2940 | Train Loss 0.2761453092098236 | Val Loss 0.6591067910194397\n",
      "Epoch 2950 | Train Loss 0.26709386706352234 | Val Loss 0.7546260356903076\n",
      "Epoch 2960 | Train Loss 0.32264894247055054 | Val Loss 0.6655701398849487\n",
      "Epoch 2970 | Train Loss 0.20477528870105743 | Val Loss 0.6467570066452026\n",
      "Epoch 2980 | Train Loss 0.3515600562095642 | Val Loss 0.7006802558898926\n",
      "Epoch 2990 | Train Loss 0.17565713822841644 | Val Loss 0.6701490879058838\n",
      "Epoch 3000 | Train Loss 0.2972151041030884 | Val Loss 0.7761140465736389\n",
      "Epoch 3010 | Train Loss 0.1907624900341034 | Val Loss 0.6704952716827393\n",
      "Epoch 3020 | Train Loss 0.2406051754951477 | Val Loss 0.7088381052017212\n",
      "Epoch 3030 | Train Loss 0.23466096818447113 | Val Loss 0.6818902492523193\n",
      "Epoch 3040 | Train Loss 0.1747339814901352 | Val Loss 0.7515956163406372\n",
      "Epoch 3050 | Train Loss 0.231043741106987 | Val Loss 0.7057653665542603\n",
      "Epoch 3060 | Train Loss 0.25719359517097473 | Val Loss 0.768880307674408\n",
      "Epoch 3070 | Train Loss 0.15787184238433838 | Val Loss 0.7109060883522034\n",
      "Epoch 3080 | Train Loss 0.17486710846424103 | Val Loss 0.6808667182922363\n",
      "Epoch 3090 | Train Loss 0.18682152032852173 | Val Loss 0.7090965509414673\n",
      "Epoch 3100 | Train Loss 0.17117281258106232 | Val Loss 0.7003079652786255\n",
      "Epoch 3110 | Train Loss 0.15338028967380524 | Val Loss 0.7520772814750671\n",
      "Epoch 3120 | Train Loss 0.12535864114761353 | Val Loss 0.6464177370071411\n",
      "Epoch 3130 | Train Loss 0.27942752838134766 | Val Loss 0.9841470718383789\n",
      "Epoch 3140 | Train Loss 0.4201927185058594 | Val Loss 0.7245899438858032\n",
      "Epoch 3150 | Train Loss 0.240874245762825 | Val Loss 0.8833317756652832\n",
      "Epoch 3160 | Train Loss 0.16280706226825714 | Val Loss 0.6893733143806458\n",
      "Epoch 3170 | Train Loss 0.17914673686027527 | Val Loss 0.7905242443084717\n",
      "Epoch 3180 | Train Loss 0.17813940346240997 | Val Loss 0.7507482767105103\n",
      "Epoch 3190 | Train Loss 0.23007936775684357 | Val Loss 0.7566519975662231\n",
      "Epoch 3200 | Train Loss 0.23248155415058136 | Val Loss 0.7743998765945435\n",
      "Epoch 3210 | Train Loss 0.1878339648246765 | Val Loss 0.6527302861213684\n",
      "Epoch 3220 | Train Loss 0.2625914514064789 | Val Loss 0.6984508037567139\n",
      "Epoch 3230 | Train Loss 0.2610463798046112 | Val Loss 0.6659365892410278\n",
      "Epoch 3240 | Train Loss 0.21308864653110504 | Val Loss 0.660759449005127\n",
      "Epoch 3250 | Train Loss 0.2691369652748108 | Val Loss 0.7980788946151733\n",
      "Epoch 3260 | Train Loss 0.16401520371437073 | Val Loss 0.7195066809654236\n",
      "Epoch 3270 | Train Loss 0.24618466198444366 | Val Loss 0.7588527798652649\n",
      "Epoch 3280 | Train Loss 0.10935504734516144 | Val Loss 0.7100173234939575\n",
      "Epoch 3290 | Train Loss 0.29069963097572327 | Val Loss 0.6399783492088318\n",
      "Epoch 3300 | Train Loss 0.26119464635849 | Val Loss 0.7075474262237549\n",
      "Epoch 3310 | Train Loss 0.22493129968643188 | Val Loss 0.7382336258888245\n",
      "Epoch 3320 | Train Loss 0.19512145221233368 | Val Loss 0.7096710801124573\n",
      "Epoch 3330 | Train Loss 0.2581506073474884 | Val Loss 0.6870567798614502\n",
      "Epoch 3340 | Train Loss 0.21315385401248932 | Val Loss 0.6000734567642212\n",
      "Epoch 3350 | Train Loss 0.127027228474617 | Val Loss 0.7577868103981018\n",
      "Epoch 3360 | Train Loss 0.1137736365199089 | Val Loss 0.6637420058250427\n",
      "Epoch 3370 | Train Loss 0.2966848909854889 | Val Loss 0.8818826079368591\n",
      "Epoch 3380 | Train Loss 0.19087231159210205 | Val Loss 0.7523813843727112\n",
      "Epoch 3390 | Train Loss 0.23159419000148773 | Val Loss 0.6931225061416626\n",
      "Epoch 3400 | Train Loss 0.19547778367996216 | Val Loss 0.7811059951782227\n",
      "Epoch 3410 | Train Loss 0.22529400885105133 | Val Loss 0.6773471832275391\n",
      "Epoch 3420 | Train Loss 0.13220098614692688 | Val Loss 0.7372920513153076\n",
      "Epoch 3430 | Train Loss 0.285023033618927 | Val Loss 0.7676159143447876\n",
      "Epoch 3440 | Train Loss 0.1429927945137024 | Val Loss 0.7149620056152344\n",
      "Epoch 3450 | Train Loss 0.23937936127185822 | Val Loss 0.6926202774047852\n",
      "Epoch 3460 | Train Loss 0.20434114336967468 | Val Loss 0.6753469705581665\n",
      "Epoch 3470 | Train Loss 0.19321833550930023 | Val Loss 0.7511525750160217\n",
      "Epoch 3480 | Train Loss 0.24703748524188995 | Val Loss 0.7134511470794678\n",
      "Epoch 3490 | Train Loss 0.2870377004146576 | Val Loss 0.6249265074729919\n",
      "Epoch 3500 | Train Loss 0.181594118475914 | Val Loss 0.6723408102989197\n",
      "Epoch 3510 | Train Loss 0.2098691761493683 | Val Loss 0.7096014618873596\n",
      "Epoch 3520 | Train Loss 0.20432186126708984 | Val Loss 0.6478111743927002\n",
      "Epoch 3530 | Train Loss 0.32248011231422424 | Val Loss 0.7191812992095947\n",
      "Epoch 3540 | Train Loss 0.15389278531074524 | Val Loss 0.7104544043540955\n",
      "Epoch 3550 | Train Loss 0.17788542807102203 | Val Loss 0.7344882488250732\n",
      "Epoch 3560 | Train Loss 0.16233406960964203 | Val Loss 0.7191334366798401\n",
      "Epoch 3570 | Train Loss 0.23285548388957977 | Val Loss 0.7719810009002686\n",
      "Epoch 3580 | Train Loss 0.24552197754383087 | Val Loss 0.702587902545929\n",
      "Epoch 3590 | Train Loss 0.13933627307415009 | Val Loss 0.6856837868690491\n",
      "Epoch 3600 | Train Loss 0.1568644642829895 | Val Loss 0.673397958278656\n",
      "Epoch 3610 | Train Loss 0.3346315920352936 | Val Loss 0.8072490692138672\n",
      "Epoch 3620 | Train Loss 0.20161986351013184 | Val Loss 0.6961584091186523\n",
      "Epoch 3630 | Train Loss 0.21617241203784943 | Val Loss 0.7406215667724609\n",
      "Epoch 3640 | Train Loss 0.1811816394329071 | Val Loss 0.6417859196662903\n",
      "Epoch 3650 | Train Loss 0.26997461915016174 | Val Loss 0.71785569190979\n",
      "Epoch 3660 | Train Loss 0.13424332439899445 | Val Loss 0.7134450078010559\n",
      "Epoch 3670 | Train Loss 0.38301602005958557 | Val Loss 0.7055938243865967\n",
      "Epoch 3680 | Train Loss 0.23539216816425323 | Val Loss 0.790596604347229\n",
      "Epoch 3690 | Train Loss 0.13798575103282928 | Val Loss 0.6926184892654419\n",
      "Epoch 3700 | Train Loss 0.2028321772813797 | Val Loss 0.7925606369972229\n",
      "Epoch 3710 | Train Loss 0.20663459599018097 | Val Loss 0.6482096910476685\n",
      "Epoch 3720 | Train Loss 0.10884507745504379 | Val Loss 0.6956123113632202\n",
      "Epoch 3730 | Train Loss 0.25334882736206055 | Val Loss 0.694575309753418\n",
      "Epoch 3740 | Train Loss 0.1797911375761032 | Val Loss 0.6462124586105347\n",
      "Epoch 3750 | Train Loss 0.26092082262039185 | Val Loss 0.8336706161499023\n",
      "Epoch 3760 | Train Loss 0.22088868916034698 | Val Loss 0.8061618804931641\n",
      "Epoch 3770 | Train Loss 0.34575068950653076 | Val Loss 0.6518326997756958\n",
      "Epoch 3780 | Train Loss 0.14334091544151306 | Val Loss 0.7156950235366821\n",
      "Epoch 3790 | Train Loss 0.13422568142414093 | Val Loss 0.663729190826416\n",
      "Epoch 3800 | Train Loss 0.18662463128566742 | Val Loss 0.6841676235198975\n",
      "Epoch 3810 | Train Loss 0.1443450003862381 | Val Loss 0.7151291966438293\n",
      "Epoch 3820 | Train Loss 0.23821254074573517 | Val Loss 0.7016504406929016\n",
      "Epoch 3830 | Train Loss 0.15166296064853668 | Val Loss 0.6753232479095459\n",
      "Epoch 3840 | Train Loss 0.10612356662750244 | Val Loss 0.7169485092163086\n",
      "Epoch 3850 | Train Loss 0.20101295411586761 | Val Loss 0.7610634565353394\n",
      "Epoch 3860 | Train Loss 0.18399231135845184 | Val Loss 0.6495159864425659\n",
      "Epoch 3870 | Train Loss 0.21394799649715424 | Val Loss 0.6576680541038513\n",
      "Epoch 3880 | Train Loss 0.19997107982635498 | Val Loss 0.6671680808067322\n",
      "Epoch 3890 | Train Loss 0.1540120393037796 | Val Loss 0.8011182546615601\n",
      "Epoch 3900 | Train Loss 0.2080453634262085 | Val Loss 0.8051804304122925\n",
      "Epoch 3910 | Train Loss 0.23935198783874512 | Val Loss 0.7170689105987549\n",
      "Epoch 3920 | Train Loss 0.22958789765834808 | Val Loss 0.7772603631019592\n",
      "Epoch 3930 | Train Loss 0.2668861746788025 | Val Loss 0.7332757711410522\n",
      "Epoch 3940 | Train Loss 0.1600133627653122 | Val Loss 0.6591504216194153\n",
      "Epoch 3950 | Train Loss 0.22268952429294586 | Val Loss 0.6631650328636169\n",
      "Epoch 3960 | Train Loss 0.13574759662151337 | Val Loss 0.6476361751556396\n",
      "Epoch 3970 | Train Loss 0.14336085319519043 | Val Loss 0.7572282552719116\n",
      "Epoch 3980 | Train Loss 0.2404402643442154 | Val Loss 0.6465219259262085\n",
      "Epoch 3990 | Train Loss 0.12578627467155457 | Val Loss 0.6579171419143677\n",
      "Epoch 4000 | Train Loss 0.18216927349567413 | Val Loss 0.6719028949737549\n",
      "Epoch 4010 | Train Loss 0.14327499270439148 | Val Loss 0.675510048866272\n",
      "Epoch 4020 | Train Loss 0.11093763262033463 | Val Loss 0.7161524891853333\n",
      "Epoch 4030 | Train Loss 0.12228870391845703 | Val Loss 0.664272665977478\n",
      "Epoch 4040 | Train Loss 0.18396885693073273 | Val Loss 0.735130786895752\n",
      "Epoch 4050 | Train Loss 0.16670037806034088 | Val Loss 0.6988129615783691\n",
      "Epoch 4060 | Train Loss 0.1640385240316391 | Val Loss 0.6681593060493469\n",
      "Epoch 4070 | Train Loss 0.26365819573402405 | Val Loss 0.7441419363021851\n",
      "Epoch 4080 | Train Loss 0.3881314694881439 | Val Loss 0.7451711893081665\n",
      "Epoch 4090 | Train Loss 0.11230533570051193 | Val Loss 0.644874632358551\n",
      "Epoch 4100 | Train Loss 0.3176537752151489 | Val Loss 0.7541712522506714\n",
      "Epoch 4110 | Train Loss 0.16181069612503052 | Val Loss 0.7229037880897522\n",
      "Epoch 4120 | Train Loss 0.1694219410419464 | Val Loss 0.714630126953125\n",
      "Epoch 4130 | Train Loss 0.13851740956306458 | Val Loss 0.730692982673645\n",
      "Epoch 4140 | Train Loss 0.09242431819438934 | Val Loss 0.754140317440033\n",
      "Epoch 4150 | Train Loss 0.17667165398597717 | Val Loss 0.7098426222801208\n",
      "Epoch 4160 | Train Loss 0.1137445941567421 | Val Loss 0.672914981842041\n",
      "Epoch 4170 | Train Loss 0.15671958029270172 | Val Loss 0.6855085492134094\n",
      "Epoch 4180 | Train Loss 0.16198351979255676 | Val Loss 0.7218923568725586\n",
      "Epoch 4190 | Train Loss 0.10990532487630844 | Val Loss 0.7453874945640564\n",
      "Epoch 4200 | Train Loss 0.12045004963874817 | Val Loss 0.7516353130340576\n",
      "Epoch 4210 | Train Loss 0.20390629768371582 | Val Loss 0.6721388697624207\n",
      "Epoch 4220 | Train Loss 0.09806349873542786 | Val Loss 0.6569810509681702\n",
      "Epoch 4230 | Train Loss 0.3168971836566925 | Val Loss 0.6389541625976562\n",
      "Epoch 4240 | Train Loss 0.21820081770420074 | Val Loss 0.7196840047836304\n",
      "Epoch 4250 | Train Loss 0.22258944809436798 | Val Loss 0.7945243716239929\n",
      "Epoch 4260 | Train Loss 0.24508564174175262 | Val Loss 0.6830033659934998\n",
      "Epoch 4270 | Train Loss 0.15012142062187195 | Val Loss 0.6255276203155518\n",
      "Epoch 4280 | Train Loss 0.21789288520812988 | Val Loss 0.6711554527282715\n",
      "Epoch 4290 | Train Loss 0.1048259288072586 | Val Loss 0.6534936428070068\n",
      "Epoch 4300 | Train Loss 0.10346317291259766 | Val Loss 0.7572526335716248\n",
      "Epoch 4310 | Train Loss 0.09280937910079956 | Val Loss 0.7266382575035095\n",
      "Epoch 4320 | Train Loss 0.22005459666252136 | Val Loss 0.7463880777359009\n",
      "Epoch 4330 | Train Loss 0.13037154078483582 | Val Loss 0.7501078844070435\n",
      "Epoch 4340 | Train Loss 0.11493151634931564 | Val Loss 0.7124398946762085\n",
      "Epoch 4350 | Train Loss 0.1700822412967682 | Val Loss 0.7427314519882202\n",
      "Epoch 4360 | Train Loss 0.1815781146287918 | Val Loss 0.7227998971939087\n",
      "Epoch 4370 | Train Loss 0.10188169777393341 | Val Loss 0.6406490206718445\n",
      "Epoch 4380 | Train Loss 0.16411051154136658 | Val Loss 0.7377712726593018\n",
      "Epoch 4390 | Train Loss 0.291238933801651 | Val Loss 0.818324089050293\n",
      "Epoch 4400 | Train Loss 0.16143673658370972 | Val Loss 0.7039068341255188\n",
      "Epoch 4410 | Train Loss 0.141007199883461 | Val Loss 0.816105306148529\n",
      "Epoch 4420 | Train Loss 0.11886117607355118 | Val Loss 0.6016160249710083\n",
      "Epoch 4430 | Train Loss 0.13138450682163239 | Val Loss 0.7264161109924316\n",
      "Epoch 4440 | Train Loss 0.21888889372348785 | Val Loss 0.7246792316436768\n",
      "Epoch 4450 | Train Loss 0.16775333881378174 | Val Loss 0.6578737497329712\n",
      "Epoch 4460 | Train Loss 0.20699825882911682 | Val Loss 0.6722113490104675\n",
      "Epoch 4470 | Train Loss 0.17745712399482727 | Val Loss 0.6953176856040955\n",
      "Epoch 4480 | Train Loss 0.11696956306695938 | Val Loss 0.6748993992805481\n",
      "Epoch 4490 | Train Loss 0.14283621311187744 | Val Loss 0.7033337950706482\n",
      "Epoch 4500 | Train Loss 0.18715675175189972 | Val Loss 0.7303152084350586\n",
      "Epoch 4510 | Train Loss 0.26301702857017517 | Val Loss 0.744685709476471\n",
      "Epoch 4520 | Train Loss 0.15152508020401 | Val Loss 0.8233349323272705\n",
      "Epoch 4530 | Train Loss 0.25930559635162354 | Val Loss 0.6975494027137756\n",
      "Epoch 4540 | Train Loss 0.25335773825645447 | Val Loss 0.6920958757400513\n",
      "Epoch 4550 | Train Loss 0.22780010104179382 | Val Loss 0.7152808904647827\n",
      "Epoch 4560 | Train Loss 0.16817235946655273 | Val Loss 0.735852837562561\n",
      "Epoch 4570 | Train Loss 0.1307043582201004 | Val Loss 0.8733952045440674\n",
      "Epoch 4580 | Train Loss 0.20817609131336212 | Val Loss 0.6379756927490234\n",
      "Epoch 4590 | Train Loss 0.08254691958427429 | Val Loss 0.7330936789512634\n",
      "Epoch 4600 | Train Loss 0.18821944296360016 | Val Loss 0.6438128352165222\n",
      "Epoch 4610 | Train Loss 0.270033061504364 | Val Loss 0.7185654640197754\n",
      "Epoch 4620 | Train Loss 0.1230088323354721 | Val Loss 0.6945514678955078\n",
      "Epoch 4630 | Train Loss 0.13517053425312042 | Val Loss 0.6931691765785217\n",
      "Epoch 4640 | Train Loss 0.22782093286514282 | Val Loss 0.6655916571617126\n",
      "Epoch 4650 | Train Loss 0.12001214921474457 | Val Loss 0.7009523510932922\n",
      "Epoch 4660 | Train Loss 0.10439592599868774 | Val Loss 0.629411518573761\n",
      "Epoch 4670 | Train Loss 0.15231749415397644 | Val Loss 0.729825496673584\n",
      "Epoch 4680 | Train Loss 0.12962880730628967 | Val Loss 0.6942334771156311\n",
      "Epoch 4690 | Train Loss 0.1012607291340828 | Val Loss 0.70667564868927\n",
      "Epoch 4700 | Train Loss 0.23559601604938507 | Val Loss 0.7124858498573303\n",
      "Epoch 4710 | Train Loss 0.1807442158460617 | Val Loss 0.7563231587409973\n",
      "Epoch 4720 | Train Loss 0.11290202289819717 | Val Loss 0.7008686065673828\n",
      "Epoch 4730 | Train Loss 0.07600861042737961 | Val Loss 0.7330070734024048\n",
      "Epoch 4740 | Train Loss 0.12431418895721436 | Val Loss 0.6323925256729126\n",
      "Epoch 4750 | Train Loss 0.1282026469707489 | Val Loss 0.769999623298645\n",
      "Epoch 4760 | Train Loss 0.23882794380187988 | Val Loss 0.7689107060432434\n",
      "Epoch 4770 | Train Loss 0.12718884646892548 | Val Loss 0.7911456227302551\n",
      "Epoch 4780 | Train Loss 0.2836953103542328 | Val Loss 0.6904580593109131\n",
      "Epoch 4790 | Train Loss 0.15416677296161652 | Val Loss 0.7368444800376892\n",
      "Epoch 4800 | Train Loss 0.2140110731124878 | Val Loss 0.7148743867874146\n",
      "Epoch 4810 | Train Loss 0.14833500981330872 | Val Loss 0.6799892783164978\n",
      "Epoch 4820 | Train Loss 0.16004888713359833 | Val Loss 0.6334484815597534\n",
      "Epoch 4830 | Train Loss 0.16488054394721985 | Val Loss 0.7189387679100037\n",
      "Epoch 4840 | Train Loss 0.07303822040557861 | Val Loss 0.6830990314483643\n",
      "Epoch 4850 | Train Loss 0.09381522238254547 | Val Loss 0.6496204733848572\n",
      "Epoch 4860 | Train Loss 0.17131991684436798 | Val Loss 0.6444235444068909\n",
      "Epoch 4870 | Train Loss 0.1893661916255951 | Val Loss 0.6596160531044006\n",
      "Epoch 4880 | Train Loss 0.3016580641269684 | Val Loss 0.8167741298675537\n",
      "Epoch 4890 | Train Loss 0.14929470419883728 | Val Loss 0.6809603571891785\n",
      "Epoch 4900 | Train Loss 0.1258302628993988 | Val Loss 0.7039453983306885\n",
      "Epoch 4910 | Train Loss 0.13817772269248962 | Val Loss 0.8251901865005493\n",
      "Epoch 4920 | Train Loss 0.11339288204908371 | Val Loss 0.7931739687919617\n",
      "Epoch 4930 | Train Loss 0.11653462052345276 | Val Loss 0.7188699841499329\n",
      "Epoch 4940 | Train Loss 0.09731820970773697 | Val Loss 0.6521739363670349\n",
      "Epoch 4950 | Train Loss 0.1733049899339676 | Val Loss 0.753376305103302\n",
      "Epoch 4960 | Train Loss 0.20905329287052155 | Val Loss 0.7199644446372986\n",
      "Epoch 4970 | Train Loss 0.13917267322540283 | Val Loss 0.7636083960533142\n",
      "Epoch 4980 | Train Loss 0.19320008158683777 | Val Loss 0.6968783736228943\n",
      "Epoch 4990 | Train Loss 0.1310240924358368 | Val Loss 0.6845303773880005\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)  \n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "data_size = len(data)\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "train_loader = DataLoader(data[:int(data_size * 0.6)], \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "val_loader = DataLoader(data[int(data_size * 0.6):int(data_size * 0.8)], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(data[int(data_size * 0.8):], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "def train(train_loader):\n",
    "    # Enumerate over the data\n",
    "    \n",
    "    for batch in train_loader:\n",
    "      # Use GPU\n",
    "      batch.to(device)  \n",
    "      # Reset gradients\n",
    "      optimizer.zero_grad() \n",
    "      # Passing the node features and the connection info\n",
    "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
    "      # Calculating the loss and gradients\n",
    "      loss = loss_fn(pred, batch.y)\n",
    "      loss.backward()  \n",
    "      # Update using the gradients\n",
    "      optimizer.step()\n",
    "    return loss, embedding\n",
    "\n",
    "def validation_loss(val_loader):\n",
    "    v_loss=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch.to(device) \n",
    "            val_pred, _ = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            val_loss = loss_fn(val_pred, batch.y) \n",
    "            v_loss.append(val_loss)\n",
    "    return  sum(v_loss)/len(v_loss)\n",
    "\n",
    "\n",
    "def test_loss(test_loader):\n",
    "    with torch.no_grad():\n",
    "        t_loss=[]\n",
    "        for batch in test_loader:\n",
    "            batch.to(device) \n",
    "            test_pred, _ = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            test_loss = loss_fn(test_pred, batch.y)\n",
    "            t_loss.append(test_loss)\n",
    "    return sum(t_loss)/len(t_loss)\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(5000):\n",
    "    train_loss, h = train(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss = validation_loss(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    if epoch % 10 == 0:\n",
    "      print(f\"Epoch {epoch} | Train Loss {train_loss} | Val Loss {val_loss}\")\n",
    "    if val_loss < 0.1 :\n",
    "        print(\"Condition Satisfied!\")\n",
    "        print(f\"Epoch {epoch} | Train Loss {train_loss} | Val Loss {val_loss}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b2a8f19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize learning (training loss)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_float = [float(loss.cpu().detach().numpy()) for loss in train_losses] \n",
    "\n",
    "val_losses_float = [float(loss.cpu().detach().numpy()) for loss in val_losses] \n",
    "\n",
    "plt.plot(train_losses_float , color = 'blue')\n",
    "plt.plot(val_losses_float , color = 'red')\n",
    "plt.legend(['Train Loss' , 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "90ad4426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y_real    y_pred\n",
      "0    0.150 -0.626235\n",
      "1   -3.931 -5.215529\n",
      "2   -2.349 -2.020530\n",
      "3   -1.620 -2.418957\n",
      "4    0.940 -1.564416\n",
      "..     ...       ...\n",
      "59  -2.090 -2.374141\n",
      "60  -3.790 -3.793498\n",
      "61  -1.790 -2.313623\n",
      "62  -4.240 -6.966203\n",
      "63  -0.800 -0.919504\n",
      "\n",
      "[64 rows x 2 columns]\n",
      "***************\n",
      "Test Loss = 0.7585558295249939\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Analyze the results for one batch\n",
    "test_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    test_batch.to(device)\n",
    "    pred, embed = model(test_batch.x.float(), test_batch.edge_index, test_batch.batch) \n",
    "    df = pd.DataFrame()\n",
    "    df[\"y_real\"] = test_batch.y.tolist()\n",
    "    df[\"y_pred\"] = pred.tolist()\n",
    "df[\"y_real\"] = df[\"y_real\"].apply(lambda row: row[0])\n",
    "df[\"y_pred\"] = df[\"y_pred\"].apply(lambda row: row[0])\n",
    "print(df)\n",
    "print(\"***************\")\n",
    "print(f\"Test Loss = {test_loss(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "526b4aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='y_real', ylabel='y_pred'>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x= np.linspace(-7,2)\n",
    "plt = sns.scatterplot(data=df, x=\"y_real\", y=\"y_pred\")\n",
    "plt.plot(x,x,color='r')\n",
    "plt.set(xlim=(-7, 2))\n",
    "plt.set(ylim=(-7, 2))\n",
    "plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
